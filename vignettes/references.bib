@article{Roberts_et_al_2019_JSTATS_91_2,
    author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
    title = {stm: An R Package for Structural Topic Models},
    journal = {Journal of Statistical Software, Articles},
    volume = {91},
    number = {2},
    year = {2019},
    keywords = {structural topic model; text analysis; LDA; stm; R},
    abstract = {This paper demonstrates how to use the R package stm for structural topic modeling. The structural topic model allows researchers to flexibly estimate a topic model that includes document-level metadata. Estimation is accomplished through a fast variational approximation. The stm package provides many useful features, including rich ways to explore topics, estimate uncertainty, and visualize quantities of interest.},
    issn = {1548-7660},
    pages = {1--40},
    doi = {10.18637/jss.v091.i02},
    url = {https://www.jstatsoft.org/v091/i02}
}


@article{RobertsStewart_et_2014_AJPS_58,
author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
title = {Structural Topic Models for Open-Ended Survey Responses},
journal = {American Journal of Political Science},
volume = {58},
number = {4},
pages = {1064-1082},
doi = {10.1111/ajps.12103},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12103},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12103},
abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
year = {2014}
}


@inproceedings{LeeMimno_2014_EMNLP_Conf,
    title = {Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference},
    author = {Lee, Moontae and Mimno, David},
    booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
    year = {2014},
    address = {Doha, Qatar},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D14-1138},
    doi = {10.3115/v1/D14-1138},
    pages = {1319--1328}
}


@inproceedings{WallachMurray_et_2009_ICML_Conf,
    author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
    title = {Evaluation Methods for Topic Models},
    year = {2009},
    isbn = {9781605585161},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1553374.1553515},
    doi = {10.1145/1553374.1553515},
    abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
    booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
    pages = {1105–1112},
    numpages = {8},
    location = {Montreal, Quebec, Canada},
    series = {ICML '09}
}


@inproceedings{Taddy_2012_AIS_Conf,
    title = {On Estimation and Selection for Topic Models},
    author = {Matt Taddy},
    pages = {1184--1193},
    year = {2012},
    editor = {Neil D. Lawrence and Mark Girolami},
    volume = {22},
    series = {Proceedings of Machine Learning Research},
    address = {La Palma, Canary Islands},
    month = {21--23 Apr},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v22/taddy12/taddy12.pdf},
    url = {http://proceedings.mlr.press/v22/taddy12.html},
    abstract = {This article describes posterior maximization for topic models, identifying computational and   conceptual gains from inference under a non-standard    parametrization.  We then show that fitted parameters can be used  as the basis for a novel approach to marginal likelihood estimation,   via block-diagonal approximation to the information matrix, that facilitates choosing the number of latent topics.  This   likelihood-based model selection is complemented with a goodness-of-fit analysis built around estimated residual dispersion.  Examples are provided to illustrate model selection as well as to compare our estimation against standard alternative techniques.}
}


@inproceedings{MimnoWallach_et_2011_EMNLP_Conf,
    author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
    title = {Optimizing Semantic Coherence in Topic Models},
    year = {2011},
    isbn = {9781937284114},
    publisher = {Association for Computational Linguistics},
    address = {USA},
    abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
    booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
    pages = {262–272},
    numpages = {11},
    location = {Edinburgh, United Kingdom},
    series = {EMNLP '11},
    url = {https://dl.acm.org/doi/10.5555/2145432.2145462},
    doi = {10.5555/2145432.2145462}
}


@inproceedings{BischofAiroldi_2012_ICML_Conf,
author = {Bischof, Jonathan M. and Airoldi, Edoardo M.},
title = {Summarizing Topical Content with Word Frequency and Exclusivity},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Recent work in text analysis commonly describes topics in terms of their most frequent words, but the exclusivity of words to topics is equally important for communicating content. We introduce Hierarchical Poisson Convolution (HPC), a model which infers regularized estimates of the differential use of words across topics as well as their frequency within topics. HPC uses known hierarchical structure on human-labeled topics to make focused comparisons of differential usage within each branch of the hierarchy of labels. We then infer a summary for each topic in terms of words that are both frequent and exclusive. We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation.},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
pages = {9–16},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12},
url = {https://dl.acm.org/doi/10.5555/3042573.3042578},
doi = {10.5555/3042573.3042578}
}


@inproceedings{SchofieldMagnusson_et_2017_ACL_Conf,
  title={Understanding text pre-processing for latent Dirichlet allocation},
  author={Schofield, Alexandra and Magnusson, M{\aa}ns and Thompson, Laure and Mimno, David},
  booktitle={Proceedings of the 15th conference of the European chapter of the Association for Computational Linguistics},
  volume={2},
  pages={432--436},
  year={2017}
}


@article{SchofieldMimno_2016_ACL_Conf,
	author = {Alexandra Schofield and David Mimno},
	title = {Comparing Apples to Apple: The Effects of Stemmers on Topic Models},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {4},
	number = {0},
	year = {2016},
	keywords = {},
	abstract = {Rule-based stemmers such as the Porter stemmer are frequently used to preprocess English corpora for topic modeling. In this work, we train and evaluate topic models on a variety of corpora using several different stemming algorithms. We examine several different quantitative measures of the resulting models, including likelihood, coherence, model stability, and entropy. Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability.},
	issn = {2307-387X},
	pages = {287--300},
	url = {https://transacl.org/ojs/index.php/tacl/article/view/868},
	pdf = {https://mimno.infosci.cornell.edu/papers/schofield_tacl_2016.pdf},
	doi = {10.1162/tacl_a_00099}
}


@article{Blei2012_COMMACM_55,
    author = {Blei, David},
    doi = {10.1145/2133806.2133826},
    issn = {1053-5888},
    journal = {Communications of the ACM},
    month = {nov},
    number = {4},
    pages = {77--84},
    title = {{Probabilistic Topic Models}},
    url = {https://dl.acm.org/doi/10.1145/2133806.2133826},
    pdf = {http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf},
    volume = {55},
    year = {2012}
}


@article{BleiNgEt2003_JMLR_3,
    author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
    file = {::},
    issn = {1532-4435},
    journal = {The Journal of Machine Learning Research},
    month = {mar},
    pages = {993--1022},
    publisher = {JMLR.org},
    title = {{Latent dirichlet allocation}},
    url = {http://dl.acm.org/citation.cfm?id=944919.944937},
    pdf = {http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf},
    volume = {3},
    year = {2003}
}


@article{McCarthyJarvis2010_BRM_42,
    abstract = {The main purpose of this study was to examine the validity of the approach to lexical diversity assessment known as the measure of textual lexical diversity (MTLD). The index for this approach is calculated as the mean length of word strings that maintain a criterion level of lexical variation. To validate the MTLD approach, we compared it against the performances of the primary competing indices in the field, which include vocd-D, TTR, Maas, Yule's K, and an HD-D index derived directly from the hypergeometric distribution function. The comparisons involved assessments of convergent validity, divergent validity, internal validity, and incremental validity. The results of our assessments of these indices across two separate corpora suggest three major findings. First, MTLD performs well with respect to all four types of validity and is, in fact, the only index not found to vary as a function of text length. Second, HD-D is a viable alternative to the vocd-D standard. And third, three of the indices--MTLD, vocd-D (or HD-D), and Maas--appear to capture unique lexical information. We conclude by advising researchers to consider using MTLD, vocd-D (or HD-D), and Maas in their studies, rather than any single index, noting that lexical diversity can be assessed in many ways and each approach may be informative as to the construct under investigation.},
    archivePrefix = {arXiv},
    arxivId = {arXiv:1011.1669v3},
    author = {McCarthy, Philip M. and Jarvis, Scott},
    doi = {10.3758/BRM.42.2.381},
    eprint = {arXiv:1011.1669v3},
    isbn = {9788578110796},
    issn = {1554-3528},
    journal = {Behavior research methods},
    keywords = {Experimental,Experimental: methods,Humans,Language Tests,Language Tests: statistics {\&} numerical data,Psychology,Reproducibility of Results},
    month = {may},
    number = {2},
    pages = {381--92},
    pmid = {20479170},
    publisher = {Springer-Verlag},
    title = {{MTLD, vocd-D, and HD-D: a validation study of sophisticated approaches to lexical diversity assessment.}},
    url = {https://link.springer.com/article/10.3758/BRM.42.2.381},
    pdf = {https://link.springer.com/content/pdf/10.3758/BRM.42.2.381.pdf},
    volume = {42},
    year = {2010}
}



